{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redis with Langchain\n",
    "\n",
    "The goal of this project is to \"talk\" with some documents using a LLM from Hugging Face, the document will be a short story from a PDF file, we will chunk that document, transform it to embedding vectors using and embedding model from Hugging Face and than store those embeddings to Redis.\n",
    "\n",
    "Than based on some questions we will do a semantic similarity search and we will retreive the documents that are similar with the question, the question is embedded using the same method. \n",
    "\n",
    "Than we will use a LLM that we take from Hugging Face, proablity \"mistral\" and we will give it some context using those retreived documents and we will generate an AI enhanced response.\n",
    "\n",
    "We need the following API keys\n",
    " - Langchanin API key\n",
    " - Hugging Face API key\n",
    " - Redis API key\n",
    "\n",
    "We need to do the following steps roughly:\n",
    " - Read the api keys form .env file\n",
    " - Search for a file (document corpus)\n",
    " - Chunkerize that document\n",
    " - Search for an embedding model and use it to vectorize the chunks\n",
    " - Upload the embeddidngs to Redis\n",
    " - Ask a question and embedd it\n",
    " - Retreive the relevant documents \n",
    " - Search for a LLM model and give it the documents as context - Text Generation model\n",
    " - The response for that question should be based on the docuemnts we have but enhanged using the LLM model \n",
    "\n",
    "\n",
    "I want to store the intermediary data in a pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "Import the necesary libraries and other stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the document\n",
    "\n",
    "Read the pdf that I generated previously, I need to import first the helper function because it is in a .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "\n",
    "# Define the path to the module\n",
    "module_path = './helpers/pdf_reader.py'\n",
    "\n",
    "# Create a module spec from the path\n",
    "spec = importlib.util.spec_from_file_location('functions', module_path)\n",
    "\n",
    "# Load the module\n",
    "functions = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file_path = './data/ion-resume.pdf'\n",
    "full_text = functions.read_pdf(pdf_file_path, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the text into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def get_recursive_text_splitter(chunk_size, chunk_overlap):\n",
    "    return RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap = chunk_overlap,\n",
    "    )\n",
    "    \n",
    "def split_documents(docs, text_splitter):\n",
    "    return  text_splitter.create_documents([docs])\n",
    "\n",
    "\n",
    "text_splitter = get_recursive_text_splitter(chunk_size=100, chunk_overlap=10)\n",
    "splitted_docs = split_documents(full_text, text_splitter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove text that has no more than x characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the text into a padas dataframe\n",
    "\n",
    "I will iterate through the splitted docs and I will assign a unique id to each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import uuid\n",
    "\n",
    "# Function to generate unique IDs\n",
    "def generate_unique_id():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "\n",
    "# Extract the page_content from each Document object into a separate list\n",
    "page_contents = [doc.page_content for doc in splitted_docs]\n",
    "\n",
    "df = pd.DataFrame(page_contents, columns=[\"chunk\"])\n",
    "\n",
    "# Generate a unique ID for each row and add it as a new column in the DataFrame\n",
    "df['unique_id'] = df.apply(lambda row: generate_unique_id(), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>unique_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ion by Liviu Rebreanu: An In-depth SummaryIon ...</td>\n",
       "      <td>9935d7dc-53fb-41ca-b59d-6528f47916e5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Ion,' authored by Liviu Rebreanu and first pu...</td>\n",
       "      <td>dde94abe-8c49-4818-9c60-e5afeff7aa4f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Romanian literature. Set against the backdrop ...</td>\n",
       "      <td>c24be74c-de07-408f-9209-27eea4dce0f4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weaves the</td>\n",
       "      <td>265f54a7-1ba8-40ed-940b-1808e1fd0ddb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>socio-economic struggles and moral dilemmas of...</td>\n",
       "      <td>e68e0280-cc25-4094-93f3-326758fd38d0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               chunk  \\\n",
       "0  Ion by Liviu Rebreanu: An In-depth SummaryIon ...   \n",
       "1  'Ion,' authored by Liviu Rebreanu and first pu...   \n",
       "2  Romanian literature. Set against the backdrop ...   \n",
       "3                                         weaves the   \n",
       "4  socio-economic struggles and moral dilemmas of...   \n",
       "\n",
       "                              unique_id  \n",
       "0  9935d7dc-53fb-41ca-b59d-6528f47916e5  \n",
       "1  dde94abe-8c49-4818-9c60-e5afeff7aa4f  \n",
       "2  c24be74c-de07-408f-9209-27eea4dce0f4  \n",
       "3  265f54a7-1ba8-40ed-940b-1808e1fd0ddb  \n",
       "4  e68e0280-cc25-4094-93f3-326758fd38d0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the text into vector embeddings\n",
    "\n",
    "Import first an embedding model and than transofrm the text from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hori\\Desktop\\Evozon\\python\\llm\\myenv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "small_embeddings_model = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "normal_embeddings_model = 'sentence-transformers/all-mpnet-base-v2'\n",
    "\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=normal_embeddings_model,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedd the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embedding'] = df['chunk'].apply(lambda text: embeddings.embed_query(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ion by Liviu Rebreanu: An In-depth SummaryIon ...</td>\n",
       "      <td>9935d7dc-53fb-41ca-b59d-6528f47916e5</td>\n",
       "      <td>[0.01243958156555891, -0.0740518867969513, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Ion,' authored by Liviu Rebreanu and first pu...</td>\n",
       "      <td>dde94abe-8c49-4818-9c60-e5afeff7aa4f</td>\n",
       "      <td>[0.018369190394878387, -0.047273170202970505, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Romanian literature. Set against the backdrop ...</td>\n",
       "      <td>c24be74c-de07-408f-9209-27eea4dce0f4</td>\n",
       "      <td>[0.013225809670984745, -0.02516471967101097, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weaves the</td>\n",
       "      <td>265f54a7-1ba8-40ed-940b-1808e1fd0ddb</td>\n",
       "      <td>[-0.011876586824655533, -0.06664972752332687, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>socio-economic struggles and moral dilemmas of...</td>\n",
       "      <td>e68e0280-cc25-4094-93f3-326758fd38d0</td>\n",
       "      <td>[-0.004901786334812641, 0.03645261377096176, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               chunk  \\\n",
       "0  Ion by Liviu Rebreanu: An In-depth SummaryIon ...   \n",
       "1  'Ion,' authored by Liviu Rebreanu and first pu...   \n",
       "2  Romanian literature. Set against the backdrop ...   \n",
       "3                                         weaves the   \n",
       "4  socio-economic struggles and moral dilemmas of...   \n",
       "\n",
       "                              unique_id  \\\n",
       "0  9935d7dc-53fb-41ca-b59d-6528f47916e5   \n",
       "1  dde94abe-8c49-4818-9c60-e5afeff7aa4f   \n",
       "2  c24be74c-de07-408f-9209-27eea4dce0f4   \n",
       "3  265f54a7-1ba8-40ed-940b-1808e1fd0ddb   \n",
       "4  e68e0280-cc25-4094-93f3-326758fd38d0   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.01243958156555891, -0.0740518867969513, -0....  \n",
       "1  [0.018369190394878387, -0.047273170202970505, ...  \n",
       "2  [0.013225809670984745, -0.02516471967101097, 0...  \n",
       "3  [-0.011876586824655533, -0.06664972752332687, ...  \n",
       "4  [-0.004901786334812641, 0.03645261377096176, 0...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store the embeddings to Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
