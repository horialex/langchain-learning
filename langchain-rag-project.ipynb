{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchian - Talk with documents\n",
    "\n",
    "The goal of this project is to \"talk\" with some documents using a LLM from Hugging Face, the document will be a short story from a PDF file, we will chunk that document, transform it to embedding vectors using and embedding model from Hugging Face and than store those embeddings to a vector store.\n",
    "\n",
    "Than based on some questions we will do a semantic similarity search and we will retreive the documents that are similar with the question, the question is embedded using the same method. \n",
    "\n",
    "Than we will use a LLM that we take from Hugging Face, proablity \"mistral\" and we will give it some context using those retreived documents and we will generate an AI enhanced response.\n",
    "\n",
    "We need the following API keys\n",
    " - Langchanin API key\n",
    " - Hugging Face API key\n",
    " - Vector store API key\n",
    "\n",
    "We need to do the following steps roughly:\n",
    " - Read the api keys form .env file\n",
    " - Search for a file (document corpus)\n",
    " - Chunkerize that document\n",
    " - Search for an embedding model and use it to vectorize the chunks\n",
    " - Upload the embeddidngs to Redis\n",
    " - Ask a question and embedd it\n",
    " - Retreive the relevant documents \n",
    " - Search for a LLM model and give it the documents as context - Text Generation model\n",
    " - The response for that question should be based on the docuemnts we have but enhanged using the LLM model \n",
    "\n",
    "\n",
    "I want to store the intermediary data in a pandas dataframe\n",
    "\n",
    "Possible vector stores:\n",
    " - Redis\n",
    " - Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "Import the necesary libraries and other stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import uuid\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the document\n",
    "\n",
    "Read the pdf that I generated previously, I need to import first the helper function because it is in a .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "\n",
    "# Define the path to the module\n",
    "module_path = './helpers/pdf_reader.py'\n",
    "\n",
    "# Create a module spec from the path\n",
    "spec = importlib.util.spec_from_file_location('functions', module_path)\n",
    "\n",
    "# Load the module\n",
    "functions = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file_path = './data/ion-resume.pdf'\n",
    "full_text = functions.read_pdf(pdf_file_path, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the text into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def get_recursive_text_splitter(chunk_size, chunk_overlap):\n",
    "    return RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap = chunk_overlap,\n",
    "    )\n",
    "    \n",
    "def split_documents(docs, text_splitter):\n",
    "    return  text_splitter.create_documents([docs])\n",
    "\n",
    "\n",
    "text_splitter = get_recursive_text_splitter(chunk_size=50, chunk_overlap=10)\n",
    "splitted_docs = split_documents(full_text, text_splitter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the text into a padas dataframe\n",
    "\n",
    "I will iterate through the splitted docs and I will assign a unique id to each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate unique IDs\n",
    "def generate_unique_id():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "\n",
    "# Extract the page_content from each Document object into a separate list\n",
    "page_contents = [doc.page_content for doc in splitted_docs]\n",
    "\n",
    "df = pd.DataFrame(page_contents, columns=[\"chunk\"])\n",
    "\n",
    "# Generate a unique ID for each row and add it as a new column in the DataFrame\n",
    "df['unique_id'] = df.apply(lambda row: generate_unique_id(), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the chunk length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['chunk_length'] = df['chunk'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the chunks smaller than a specific margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin = 40\n",
    "df = df[df['chunk_length'] >= margin]\n",
    "\n",
    "# Drop the chunk_length column as it's no longer needed\n",
    "df = df.drop(columns=['chunk_length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the text into vector embeddings\n",
    "\n",
    "Import first an embedding model and than transofrm the text from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "small_embeddings_model = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "normal_embeddings_model = 'sentence-transformers/all-mpnet-base-v2'\n",
    "\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=normal_embeddings_model,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedd the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embeddings'] = df['chunk'].apply(lambda text: embeddings.embed_query(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ion by Liviu Rebreanu: An In-depth SummaryIon by</td>\n",
       "      <td>1ea6c1f2-0158-47d1-b00c-f2eb153d5e26</td>\n",
       "      <td>[0.011528627015650272, -0.07288378477096558, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'Ion,' authored by Liviu Rebreanu and first</td>\n",
       "      <td>1d5069a4-acdc-43de-a5f4-b3dfaf0da138</td>\n",
       "      <td>[0.02417871728539467, -0.05120783671736717, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and first published in 1920, stands as a</td>\n",
       "      <td>90633971-ebf5-4783-a3b5-0393ac8e4915</td>\n",
       "      <td>[0.030614905059337616, -0.01263571996241808, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Romanian literature. Set against the backdrop of</td>\n",
       "      <td>46963f0b-b643-4429-b06b-ad34f5c98c8b</td>\n",
       "      <td>[0.029097026214003563, 0.03540997579693794, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>of rural Transylvania, the novel intricately</td>\n",
       "      <td>65ab013e-4ab9-4876-a3d6-1ad612ba5e5c</td>\n",
       "      <td>[0.04430192708969116, 0.0027177438605576754, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              chunk  \\\n",
       "0  Ion by Liviu Rebreanu: An In-depth SummaryIon by   \n",
       "3       'Ion,' authored by Liviu Rebreanu and first   \n",
       "4          and first published in 1920, stands as a   \n",
       "6  Romanian literature. Set against the backdrop of   \n",
       "7      of rural Transylvania, the novel intricately   \n",
       "\n",
       "                              unique_id  \\\n",
       "0  1ea6c1f2-0158-47d1-b00c-f2eb153d5e26   \n",
       "3  1d5069a4-acdc-43de-a5f4-b3dfaf0da138   \n",
       "4  90633971-ebf5-4783-a3b5-0393ac8e4915   \n",
       "6  46963f0b-b643-4429-b06b-ad34f5c98c8b   \n",
       "7  65ab013e-4ab9-4876-a3d6-1ad612ba5e5c   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [0.011528627015650272, -0.07288378477096558, -...  \n",
       "3  [0.02417871728539467, -0.05120783671736717, -0...  \n",
       "4  [0.030614905059337616, -0.01263571996241808, 0...  \n",
       "6  [0.029097026214003563, 0.03540997579693794, -0...  \n",
       "7  [0.04430192708969116, 0.0027177438605576754, -...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store the embeddings to Vector Store - Pinecone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the conection tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hugging_face_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "langchain_token = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pinecone_env = os.getenv(\"PINECONE_ENV\")\n",
    "pinecone_index_host = os.getenv(\"PINECONE_INDEX_HOST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get embeddign dimensions for the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "first_embedding = df['embeddings'].iloc[0]\n",
    "embeddings_dimension = len(first_embedding)\n",
    "\n",
    "print(embeddings_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to Pinecone and create the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "import time\n",
    "\n",
    "index_name = \"ion-index\"\n",
    "\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=embeddings_dimension,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the review texts to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['chunk'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push the embeddings to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "vstore = PineconeVectorStore.from_texts(texts, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"- Vanessa: Ion's true love, a beautiful but\"), Document(page_content='unattainable love and happiness for Ion. His'), Document(page_content=\"friend of Ion, providing a contrast to Ion's\"), Document(page_content=\"Ion's character embodies the complex interplay\"), Document(page_content=\"Ion's relentless pursuit culminates in his\"), Document(page_content=\"'Ion' a timeless work that continues to resonate\"), Document(page_content='all-consuming obsession. Ion recognizes that'), Document(page_content=\"sacrifice. Ana's love for Ion and her subsequent\"), Document(page_content='follows the life of Ion, a poor but ambitious'), Document(page_content='Ion. His relentless pursuit of wealth and social')]\n"
     ]
    }
   ],
   "source": [
    "query = \"Who is Ion's true love?\"\n",
    "emb_query = embeddings.embed_query(query)\n",
    "\n",
    "result = vstore.similarity_search(emb_query, k=10)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate a LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\Hori\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Initialize the HuggingFaceEndpoint\n",
    "chat_llm = HuggingFaceEndpoint(repo_id=repo_id,\n",
    "                          temperature=0.1,\n",
    "                          huggingfacehub_api_token=hugging_face_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a chain where I will pass the extracted documents by the simmilarity search to the llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In the novel \"Ion\" by Liviu Rebreanu, the character Ion is deeply in love with a woman named Smaranda. Their love story is a central theme of the novel.\n"
     ]
    }
   ],
   "source": [
    "text_chain = RetrievalQA.from_chain_type(llm=chat_llm, chain_type=\"stuff\", retriever=vstore.as_retriever())\n",
    "\n",
    "q=\"\"\"In the context of the romanian novel Ion, written by the romanian writer Liviu Rebreanu who is Ion's true love?\n",
    "\"\"\"\n",
    "\n",
    "result=text_chain.run(q)\n",
    "print(result)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a prompt where I will ask the llm a question related to my document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "import textwrap\n",
    "\n",
    "\n",
    "def perform_similarity_search(query, top_k=5):\n",
    "    search_results = vstore.similarity_search(query, top_k)\n",
    "    return search_results\n",
    "\n",
    "def answer_question_with_context(question, similar_docs):\n",
    "    # Extract the text of similar documents and form the context\n",
    "    context = \" \".join([doc.page_content for doc in similar_docs])\n",
    "    \n",
    "    system_prompt = \"You are an expert on Romanian literature. Please provide detailed and accurate answers based on the provided context.\"\n",
    "    input_text = f\"{system_prompt}\\n\\nContext: {context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    \n",
    "    messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=input_text),\n",
    "    ]\n",
    "\n",
    "    response = chat_llm.invoke(messages)\n",
    "    answer = textwrap.fill(response, width=100)\n",
    "    \n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     In the context provided, Ion's true love is Vanessa. The text suggests that Ion has deep and\n",
      "unattainable feelings for Vanessa, which he pursues relentlessly. Despite the challenges, Ion's love\n",
      "for Vanessa remains a significant aspect of his character and drives much of the narrative.\n"
     ]
    }
   ],
   "source": [
    "q = \"\"\"\n",
    "    who is Ion's true love?\n",
    "    \"\"\"\n",
    "\n",
    "similar_docs = perform_similarity_search(q)\n",
    "result = answer_question_with_context(q, similar_docs)\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
