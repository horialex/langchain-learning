{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query a book \n",
    "\n",
    "#### Pinecone Vector Store\n",
    "\n",
    "Basically I want to ask questions and get the response back from the llm that will use a book for context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hugging_face_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "langchain_token = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pinecone_env = os.getenv(\"PINECONE_ENV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\Hori\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "\n",
    "llm = HuggingFaceEndpoint(repo_id=repo_id,\n",
    "                          huggingfacehub_api_token=hugging_face_token,\n",
    "                          temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 1221 0 (offset 0)\n",
      "Ignoring wrong pointing object 1309 0 (offset 0)\n",
      "Ignoring wrong pointing object 1388 0 (offset 0)\n",
      "Ignoring wrong pointing object 1412 0 (offset 0)\n",
      "Ignoring wrong pointing object 2082 0 (offset 0)\n",
      "Ignoring wrong pointing object 2429 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"../../data/field-guide-to-data-science.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 126 document(s) in your data\n",
      "There are 0 characters in your sample document\n",
      "Here is a sample: \n"
     ]
    }
   ],
   "source": [
    "# Note: If you're using PyPDFLoader then it will split by page for you already\n",
    "print (f'You have {len(data)} document(s) in your data')\n",
    "print (f'There are {len(data[0].page_content)} characters in your sample document')\n",
    "print (f'Here is a sample: {data[0].page_content[:150]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the text into smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many small chunks we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 258 documents\n"
     ]
    }
   ],
   "source": [
    "print (f'Now you have {len(texts)} documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 258 documents that have an average of 786  characters (smaller pieces)\n"
     ]
    }
   ],
   "source": [
    "num_total_characters = sum([len(x.page_content) for x in texts])\n",
    "print(f\"Now you have {len(texts)} documents that have an average of {num_total_characters / len(texts):,.0f}  characters (smaller pieces)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create embeddings and store them to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hori\\Desktop\\Evozon\\python\\llm\\myenv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone as LangchainPinecone\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "pc = Pinecone(\n",
    "        api_key=os.environ.get(\"PINECONE_API_KEY\")\n",
    "    )\n",
    "\n",
    "index_name=\"langchain1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the index in Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=768,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the embeddings to pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = LangchainPinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are examples of good data science teams?\"\n",
    "docs = docsearch.similarity_search(query=query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Data Science teams need a broad view of the organization. Leaders \\nmust be key advocates who meet with stakeholders to ferret out \\nthe hardest challenges, locate the data, connect disparate parts of \\nthe business, and gain widespread buy-in.››\\n17 The Short Version 17'), Document(page_content='43 Start Here for the Basics 43 Start Here for the BasicsShaping the Culture\\nIt is no surprise—building a culture is hard and there is just as \\nmuch art to it as there is science. It is about deliberately creating the \\nconditions for Data Science to flourish (for both Data Scientists and \\nthe average employee). You can then step back to empower collective \\nownership of an organic transformation. \\nData Scientists are fundamentally curious and imaginative. We have \\na saying on our team, “We’re not nosy, we’re Data Scientists.” These \\nqualities are fundamental to the success of the project and to gaining \\nnew dimensions on challenges and questions. Often Data Science \\nprojects are hampered by the lack of the ability to imagine something \\nnew and different. Fundamentally, organizations must foster trust and \\ntransparent communication across all levels, instead of deference to \\nauthority, in order to establish a strong Data Science team. Managers'), Document(page_content='team, but not manage it. \\nDon’t bog them down with \\nresponsibilities of management \\nthat could be done by other staff.\\n ›Put extr a effort into managing \\ntheir careers and interests \\nwithin your organization. Build \\nopportunities for promotion into \\nyour organization that allow \\nthem to focus on mentoring other \\nData Scientists and progressing \\nthe state of the art while also \\nadvancing their careers.\\n ›Make sure that the y have the \\nopportunity to present and \\nspread their ideas in many \\ndifferent forums, but also be \\nsensitive to their time.\\n Source: Booz Allen Hamilton'), Document(page_content='at last, the way forward is identified. It requires a unique set of \\npersonality attributes to succeed in such an environment. Technical \\nskills can be developed over time: the ability to be flexible – and \\npatient, and persistent – cannot.\\nFinding the Athletes for Y our T eam\\nBuilding a Data Science team is complex. Organizations must \\nsimultaneously engage existing internal staff to create an “anchor” who \\ncan be used to recruit and grow the team, while at the same time \\nundergo organizational change and transformation to meaningfully \\nincorporate this new class of employee. Building a team starts with \\nidentifying existing staff within an organization who have a high \\naptitude for Data Science. Good candidates will have a formal \\nbackground in any of the three foundational technical skills we \\nmentioned, and will most importantly have the personality traits \\nnecessary for Data Science. They may often have advanced (masters or'), Document(page_content='will identify and select Data Science projects. \\nDeployed Data Science  teams go to the business unit and reside there for \\nshort- or long-term assignments. They are their own entity and they work \\nwith the domain experts within the group to solve hard problems. In \\nthe deployed model, Data Science teams collectively develop knowledge \\nacross business units, with central leadership as a bridging mechanism for \\naddressing organization-wide issues. However, Data Science teams are \\naccountable to business unit leadership and their centralized leadership, \\nwhich could cause confusion and conflict. In this model, it is important  \\nto emphasize conflict management to avoid competing priorities. \\nThe Diffused Data Science  team is one that is fully embedded with each \\ngroup and becomes part of the long-term organization. These teams work \\nbest when the nature of the domain or business unit is already one focused \\non analytics. In the Diffused Model, teams can quickly react to high-')]\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the documents and get the answer back from the LLM based on those returned docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "chain = load_qa_chain(llm,  chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are The Stages of Data Science Maturity?\"\n",
    "docs = docsearch.similarity_search(query=query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The Stages of Data Science Maturity are Collect, Describe, Discover, Predict, and Advise. Each stage represents an increasing level of maturity and analytic capability. The Collect stage focuses on collecting internal or external datasets. The Describe stage seeks to enhance or refine raw data and leverage basic analytic functions. The Discover stage identifies hidden relationships or patterns. The Predict stage utilizes past observations to make predictions. The Advise stage applies insights gained from data analysis to inform decision making. The proportion of time spent on each stage changes as an organization matures, with less time spent on earlier stages and more time spent on later, more mature stages.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(input_documents=docs, question=query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
