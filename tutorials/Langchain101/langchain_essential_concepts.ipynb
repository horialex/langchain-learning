{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Cookbook\n",
    "\n",
    "Tutorial link:\n",
    "\n",
    "https://www.youtube.com/watch?v=2xxziIWmaSA&list=PLqZXAkvF1bPNQER9mLmDbntNfSpzdDIU5&index=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hugging_face_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "langchain_token = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "serp_token = os.getenv(\"SERPAPI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\Hori\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "\n",
    "llm = HuggingFaceEndpoint(repo_id=repo_id,\n",
    "                          huggingfacehub_api_token=hugging_face_token,\n",
    "                          temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text - The natural language to interact with LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text =\"Tell me what day comes after Friday. Tell me only the name of the day\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "The day after Friday is Saturday.\n",
      "\n",
      "Sometimes people confuse this question with \"What is the next day after today, given it's a Friday?\" In that case, the answer would be \"Sunday,\" assuming today is a Friday. However, in the question above, we are asked for the name of the day that follows Friday, regardless of what day today is. Therefore, the answer is always Saturday.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(my_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\Hori\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.03,\n",
    ")\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hori\\Desktop\\Evozon\\python\\llm\\myenv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='There are many delicious dishes that feature tomatoes! Here are a few ideas:\\n\\n1. Bruschetta: Grilled or toasted bread topped with chopped tomatoes, garlic, olive oil, and basil.\\n\\n2. Caprese salad: Sliced tomatoes, fresh mozzarella, and basil leaves arranged in a salad with olive oil, balsamic vinegar, and salt.\\n\\n3. Gazpacho: A cold', response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=100, prompt_tokens=64, total_tokens=164), 'model': '', 'finish_reason': 'length'}, id='run-6d00ce03-ea91-4091-ad00-01758f88a6d4-0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a nice AI bot that helps user to figure out what to eat\"),\n",
    "        HumanMessage(content=\"I like tomatos, what should I eat?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass chat history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Nice, besides its stunning beaches, also offers a variety of other attractions. Here are some suggestions:\\n\\n1. Old Town (Vieille Ville): Explore the narrow streets, colorful buildings, and elegant shops of this beautifully preserved medieval area. Don't miss the Chapelle de la Miséricorde and the Cours Saleya market.\\n\\n2. Promenade des Anglais: Take a stroll along this famous waterfront walkway\", response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=100, prompt_tokens=111, total_tokens=211), 'model': '', 'finish_reason': 'length'}, id='run-12cff862-b4ca-42ca-95a1-3b99d356ee1f-0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a nice AI bot that helps user to figure out where to travel in one short sentance\"),\n",
    "        HumanMessage(content=\"I like the beaches, where should I go?\"),\n",
    "        SystemMessage(content=\"You should go to Nice, France\"),\n",
    "        HumanMessage(content=\"What else should I do when I'm there?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'my_document_id': 234234, 'my_document_source': 'The langchain Papers', 'my_document_create_time': 1680013019}, page_content='There is my document. It is full text that I have gathered from other places')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document(page_content=\"There is my document. It is full text that I have gathered from other places\",\n",
    "         metadata={\n",
    "            'my_document_id': 234234,\n",
    "            'my_document_source': \"The langchain Papers\",\n",
    "            'my_document_create_time': 1680013019   \n",
    "         })\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The statement \"Today is Monday, tomorrow is Wednesday\" is incorrect because there are 2 days (Tuesday and Thursday) missing between Monday and Wednesday. The correct statement would be \"Today is Monday, tomorrow is Tuesday, and the day after tomorrow is Wednesday.\" Alternatively, the statement \"Today is Monday, in 2 days it will be Wednesday\" would be correct.', response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=80, prompt_tokens=45, total_tokens=125), 'model': '', 'finish_reason': 'eos_token'}, id='run-20e80d1f-e1a9-4523-9e2f-ed1074f7748d-0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "    Today is Monday, tomorrow is Wednesday.\n",
    "    \n",
    "    What is wrong with that statement?\n",
    "\"\"\"\n",
    "\n",
    "chat_model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Prompt: \n",
      "    I really want to travel to Madrid. What should I do there?\n",
      "    Respond in one short sentence.\n",
      "\n",
      "----------------\n",
      "LLM Output: content=\"Experience the vibrant culture, delicious cuisine, and historic sights of Spain's lively capital in Madrid.\" response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=26, prompt_tokens=49, total_tokens=75), 'model': '', 'finish_reason': 'eos_token'} id='run-4da14002-0704-4bc0-87c0-790afbfc1f9f-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "    I really want to travel to {location}. What should I do there?\n",
    "    Respond in one short sentence.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"location\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(location=\"Madrid\")\n",
    "\n",
    "print(f\"Final Prompt: {final_prompt}\")\n",
    "print(\"----------------\")\n",
    "print(f\"LLM Output: {chat_model.invoke(final_prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Selectors\n",
    "\n",
    "We show the language model how to respond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Example Input: {input}\\n Example Output: {output}\"\n",
    ")\n",
    "\n",
    "# Example of locations where nouns are found\n",
    "examples = [\n",
    "    {\"input\": \"pirate\", \"output\": \"ship\"},\n",
    "    {\"input\": \"pilot\", \"output\": \"plane\"},\n",
    "    {\"input\": \"driver\", \"output\": \"car\"},\n",
    "    {\"input\": \"tree\", \"output\": \"forest\"},\n",
    "    {\"input\": \"bird\", \"output\": \"nest\"},\n",
    "    {\"input\": \"cloud\", \"output\": \"sky\"},\n",
    "    {\"input\": \"teacher\", \"output\": \"school\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples,\n",
    "    #The embeddings model\n",
    "    HuggingFaceEmbeddings(),\n",
    "    # The vector store class where we will store the embeddings\n",
    "    FAISS,\n",
    "    k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_prompt = FewShotPromptTemplate(\n",
    "    #The object that will help select examples\n",
    "    example_selector=example_selector,\n",
    "    \n",
    "    # Your prompt\n",
    "    example_prompt=example_prompt,\n",
    "    \n",
    "    # Customizations that will be added on top and bottom of your prompt\n",
    "    prefix=\"Give the location an item that is usually found in\",\n",
    "    suffix=\"Input: {noun}\\n Output:\",\n",
    "    \n",
    "    #What inputs your prompt will receive\n",
    "    input_variables=[\"noun\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the location an item that is usually found in\n",
      "\n",
      "Example Input: teacher\n",
      " Example Output: school\n",
      "\n",
      "Example Input: pirate\n",
      " Example Output: ship\n",
      "\n",
      "Input: student\n",
      " Output:\n"
     ]
    }
   ],
   "source": [
    "my_noun = \"student\"\n",
    "\n",
    "print(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' classroom\\n\\nInput: chef\\n Output: kitchen\\n\\nInput: engineer\\n Output: factory\\n\\nInput: athlete\\n Output: gymnasium\\n\\nInput: astronaut\\n Output: space station\\n\\nInput: doctor\\n Output: hospital\\n\\nInput: architect\\n Output: construction site\\n\\nInput: artist\\n Output: art studio\\n\\nInput: farmer\\n Output: farm\\n\\nInput: accountant\\n Output: office\\n\\nInput: pilot\\n Output: airplane\\n\\nInput: lawyer\\n Output: courthouse\\n\\nInput: soldier\\n Output: military base\\n\\nInput: musician\\n Output: music studio\\n\\nInput: carpenter\\n Output: woodworking shop\\n\\nInput: firefighter\\n Output: fire station\\n\\nInput: nurse\\n Output: hospital\\n\\nInput: police officer\\n Output: police station\\n\\nInput: journalist\\n Output: newsroom\\n\\nInput: veterinarian\\n Output: animal hospital\\n\\nInput: judge\\n Output: courthouse\\n\\nInput: librarian\\n Output: library\\n\\nInput: scientist\\n Output: laboratory\\n\\nInput: professor\\n Output: university\\n\\nInput: politician\\n Output: government building\\n\\nInput: astronomer\\n Output: observatory\\n\\nInput: environmentalist\\n Output: nature preserve\\n\\nInput: historian\\n Output: museum\\n\\nInput: archaeologist\\n Output: dig site\\n\\nInput: chef\\n Output: kitchen\\n\\nInput: baker\\n Output: bakery\\n\\nInput: farmer\\n Output: greenhouse\\n\\nInput: painter\\n Output: art gallery\\n\\nInput: sculptor\\n Output: studio\\n\\nInput: writer\\n Output: library or study\\n\\nInput: athlete\\n Output: gym or sports arena\\n\\nInput: artist\\n Output: studio or art gallery\\n\\nInput: musician\\n Output: recording studio or concert venue\\n\\nInput: engineer\\n Output: factory or research lab\\n\\nInput: architect\\n Output: construction site or office\\n\\nInput: teacher\\n Output: school or classroom\\n\\nInput: student\\n Output: classroom or library\\n\\nInput: carpenter\\n Output: workshop or construction site\\n\\nInput: doctor\\n Output: hospital or clinic\\n\\nInput: nurse\\n Output: hospital or clinic\\n\\nInput: lawyer\\n Output: law firm or courthouse\\n\\nInput: judge\\n Output: courthouse\\n\\nInput: police officer\\n Output: police station or beat'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " classroom\n",
      "\n",
      "Input: chef\n",
      " Output: kitchen\n",
      "\n",
      "Input: engineer\n",
      " Output: factory\n",
      "\n",
      "Input: athlete\n",
      " Output: gymnasium\n",
      "\n",
      "Input: astronaut\n",
      " Output: space station\n",
      "\n",
      "Input: doctor\n",
      " Output: hospital\n",
      "\n",
      "Input: architect\n",
      " Output: construction site\n",
      "\n",
      "Input: artist\n",
      " Output: art studio\n",
      "\n",
      "Input: farmer\n",
      " Output: farm\n",
      "\n",
      "Input: accountant\n",
      " Output: office\n",
      "\n",
      "Input: pilot\n",
      " Output: airplane\n",
      "\n",
      "Input: lawyer\n",
      " Output: courthouse\n",
      "\n",
      "Input: soldier\n",
      " Output: military base\n",
      "\n",
      "Input: musician\n",
      " Output: music studio\n",
      "\n",
      "Input: carpenter\n",
      " Output: woodworking shop\n",
      "\n",
      "Input: firefighter\n",
      " Output: fire station\n",
      "\n",
      "Input: nurse\n",
      " Output: hospital\n",
      "\n",
      "Input: police officer\n",
      " Output: police station\n",
      "\n",
      "Input: journalist\n",
      " Output: newsroom\n",
      "\n",
      "Input: veterinarian\n",
      " Output: animal hospital\n",
      "\n",
      "Input: judge\n",
      " Output: courthouse\n",
      "\n",
      "Input: librarian\n",
      " Output: library\n",
      "\n",
      "Input: scientist\n",
      " Output: laboratory\n",
      "\n",
      "Input: professor\n",
      " Output: university\n",
      "\n",
      "Input: politician\n",
      " Output: government building\n",
      "\n",
      "Input: astronomer\n",
      " Output: observatory\n",
      "\n",
      "Input: environmentalist\n",
      " Output: nature preserve\n",
      "\n",
      "Input: historian\n",
      " Output: museum\n",
      "\n",
      "Input: archaeologist\n",
      " Output: dig site\n",
      "\n",
      "Input: chef\n",
      " Output: kitchen\n",
      "\n",
      "Input: baker\n",
      " Output: bakery\n",
      "\n",
      "Input: farmer\n",
      " Output: greenhouse\n",
      "\n",
      "Input: painter\n",
      " Output: art gallery\n",
      "\n",
      "Input: sculptor\n",
      " Output: studio\n",
      "\n",
      "Input: writer\n",
      " Output: library or study\n",
      "\n",
      "Input: athlete\n",
      " Output: gym or sports arena\n",
      "\n",
      "Input: artist\n",
      " Output: studio or art gallery\n",
      "\n",
      "Input: musician\n",
      " Output: recording studio or concert venue\n",
      "\n",
      "Input: engineer\n",
      " Output: factory or research lab\n",
      "\n",
      "Input: architect\n",
      " Output: construction site or office\n",
      "\n",
      "Input: teacher\n",
      " Output: school or classroom\n",
      "\n",
      "Input: student\n",
      " Output: classroom or library\n",
      "\n",
      "Input: carpenter\n",
      " Output: workshop or construction site\n",
      "\n",
      "Input: doctor\n",
      " Output: hospital or clinic\n",
      "\n",
      "Input: nurse\n",
      " Output: hospital or clinic\n",
      "\n",
      "Input: lawyer\n",
      " Output: law firm or courthouse\n",
      "\n",
      "Input: judge\n",
      " Output: courthouse\n",
      "\n",
      "Input: police officer\n",
      " Output: police station or beat\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "raw_output = llm.invoke(similar_prompt.format(noun=my_noun))\n",
    "# Use the parser to extract the final answer\n",
    "parsed_output = parser.parse(raw_output)\n",
    "\n",
    "# Print the result\n",
    "print(parsed_output) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# How you would like your response structured. This is basically a fancy prompt template\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"bad_string\", description=\"This a poorly formatted user input string\"),\n",
    "    ResponseSchema(name=\"good_string\", description=\"This is your response, a reformatted response\")\n",
    "]\n",
    "\n",
    "# How you would like to parse your output\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response, a reformatted response\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# See the prompt template you created for formatting\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print (format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You will be given a poorly formatted string from a user.\n",
      "Reformat it and make sure all the words are spelled correctly\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response, a reformatted response\n",
      "}\n",
      "```\n",
      "\n",
      "% USER INPUT:\n",
      "welcom to califonya!\n",
      "\n",
      "YOUR RESPONSE:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "You will be given a poorly formatted string from a user.\n",
    "Reformat it and make sure all the words are spelled correctly\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "promptValue = prompt.format(user_input=\"welcom to califonya!\")\n",
    "\n",
    "print(promptValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n\\t\"bad_string\": \"welcome to california!\",\\n\\t\"good_string\": \"Welcome to California!\"\\n}\\n```\\n\\nExplanation:\\nWe simply capitalized the first letter of each word to make it a grammatically correct sentence.\\n\\nThis can be automated using regular expressions or string manipulation libraries in your preferred language.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_output = llm.invoke(promptValue)\n",
    "llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bad_string': 'welcome to california!',\n",
       " 'good_string': 'Welcome to California!'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(llm_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains\n",
    "\n",
    "#### Simple Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hori\\Desktop\\Evozon\\python\\llm\\myenv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
    "% USER LOCATION\n",
    "{user_location}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"user_location\"],\n",
    "    template=template)\n",
    "\n",
    "# Holds my 'location' chain\n",
    "location_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given a meal, give a short and simple recipe on how to make that dish at home.\n",
    "% MEAL\n",
    "{user_meal}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
    "\n",
    "# Holds my 'meal' chain\n",
    "meal_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "Classic Dish: Beef Wellington\n",
      "\n",
      "Ingredients:\n",
      "- 1 kg beef fillet\n",
      "- 2 tbsp olive oil\n",
      "- 1 onion, finely chopped\n",
      "- 2 cloves garlic, minced\n",
      "- 250 g mushrooms, finely chopped\n",
      "- 3 tbsp chopped fresh parsley\n",
      "- 3 tbsp chopped fresh thyme\n",
      "- Salt and black pepper, to taste\n",
      "- 500 g puff pastry, chilled\n",
      "- 1 egg, beaten\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Preheat the oven to 220°C/425°F/gas mark 7.\n",
      "\n",
      "2. Heat the olive oil in a frying pan over medium-high heat. Add the beef fillet and cook for 3-4 minutes on each side until browned. Remove from the pan and set aside.\n",
      "\n",
      "3. In the same pan, add the onion and garlic and cook until soft. Add the mushrooms and cook until they release their moisture and turn brown. Stir in the parsley and thyme. Season with salt and pepper.\n",
      "\n",
      "4. Place the cooked beef fillet in the centre of a large piece of cling film. Spread the mushroom mixture over the beef, pressing down firmly to pack it in.\n",
      "\n",
      "5. Roll out the pastry to form a rectangle large enough to enclose the beef. Place the pastry over the beef and wrap it tightly around the beef, pressing the edges together to seal. Brush the top of the pastry with the beaten egg.\n",
      "\n",
      "6. Place the wrapped beef on a baking sheet and bake for 30-35 minutes or until the pastry is golden brown.\n",
      "\n",
      "7. Allow the beef Wellington to rest for 10 minutes before slicing and serving.\n",
      "\n",
      "Enjoy your classic London dish!\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "This classic dish, Beef Wellington, is a true delight for any food lover. The combination of tender beef and rich, buttery pastry creates a mouth-watering flavour that will leave you wanting more.\n",
      "\n",
      "To make this dish at home, start by preheating your oven to 220°C/425°F/gas mark 7. Next, heat some olive oil in a frying pan and sear the beef fillet until it's browned on all sides. Once cooked, remove the beef from the pan and set it aside.\n",
      "\n",
      "Next, in the same frying pan, sauté some finely chopped onions and garlic until they're soft. Then add some finely chopped mushrooms and continue cooking until they've released their moisture and turned brown. Stir in some chopped parsley and thyme, and season with salt and pepper to taste.\n",
      "\n",
      "Place the cooked beef fillet in the centre of a large piece of cling film, and spread the mushroom mixture over the beef, pressing it down firmly to pack it in.\n",
      "\n",
      "Roll out a chilled piece of puff pastry to form a rectangle that's large enough to enclose the beef. Carefully place the pastry over the beef and wrap it tightly around the beef, pressing the edges together to seal. Brush the top of the pastry with a beaten egg.\n",
      "\n",
      "Place the wrapped beef on a baking sheet and bake it in the preheated oven for 30-35 minutes, or until the pastry is golden brown.\n",
      "\n",
      "Once the Beef Wellington is done, allow it to rest for 10 minutes before slicing and serving. This will help the juices redistribute, making for a juicier, more delicious dish.\n",
      "\n",
      "Overall, this recipe is straightforward and easy to follow, perfect for anyone looking to impress their guests with a classic London dish. Enjoy!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "review = overall_chain.invoke(\"London\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarization Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = TextLoader('../../data/paul_essay.txt')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarization chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"January 2017Because biographies of famous scientists tend to \n",
      "edit out their mistakes, we underestimate the \n",
      "degree of risk they were willing to take.\n",
      "And because anything a famous scientist did that\n",
      "wasn't a mistake has probably now become the\n",
      "conventional wisdom, those choices don't\n",
      "seem risky either.Biographies of Newton, for example, understandably focus\n",
      "more on physics than alchemy or theology.\n",
      "The impression we get is that his unerring judgment\n",
      "led him straight to truths no one else had noticed.\n",
      "How to explain all the time he spent on alchemy\n",
      "and theology?  Well, smart people are often kind of\n",
      "crazy.But maybe there is a simpler explanation. Maybe\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"the smartness and the craziness were not as separate\n",
      "as we think. Physics seems to us a promising thing\n",
      "to work on, and alchemy and theology obvious wastes\n",
      "of time. But that's because we know how things\n",
      "turned out. In Newton's day the three problems \n",
      "seemed roughly equally promising. No one knew yet\n",
      "what the payoff would be for inventing what we\n",
      "now call physics; if they had, more people would \n",
      "have been working on it. And alchemy and theology\n",
      "were still then in the category Marc Andreessen would \n",
      "describe as \"huge, if true.\"Newton made three bets. One of them worked. But \n",
      "they were all risky.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5ecbc248cf46079c48f86b21d34f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hori\\Desktop\\Evozon\\python\\llm\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Hori\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e38701a3d5447aaab76347ddb56492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c70dd1e203b46d9ab642097aad3d2ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a1e8f419604aba844b4c7a07108e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f760cdf5a723474092f9638a5e612dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\" The tendency to highlight the successes of famous scientists in their biographies leads to an underestimation of the risks they took. Their decisions that did not result in mistakes have become conventional wisdom, making them appear less risky. Biographies often focus more on a scientist's successful contributions to their field, making it seem that their unerring judgment led them directly to previously unnoticed truths. However, the time spent by these scientists on other pursuits, such as alchemy and theology, may suggest that they were not always rational in their decision-making. A simpler explanation could be that successful people sometimes make questionable choices.\n",
      "\n",
      " The author highlights the similarity between seemingly disparate pursuits such as smartness and craziness, pointing out that what we consider promising or wasteful is often subjective and dependent on the outcome. In Newton's time, all three fields - physics, alchemy, and theology - seemed equally promising, as their potential payoffs were unknown. Newton's bets in these areas were all risky, and only one paid off. This underscores the importance of taking risks and the unpredictability of outcomes in any pursuit, whether considered smart or crazy.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(metadata={'source': '../../data/paul_essay.txt'}, page_content=\"January 2017Because biographies of famous scientists tend to \\nedit out their mistakes, we underestimate the \\ndegree of risk they were willing to take.\\nAnd because anything a famous scientist did that\\nwasn't a mistake has probably now become the\\nconventional wisdom, those choices don't\\nseem risky either.Biographies of Newton, for example, understandably focus\\nmore on physics than alchemy or theology.\\nThe impression we get is that his unerring judgment\\nled him straight to truths no one else had noticed.\\nHow to explain all the time he spent on alchemy\\nand theology?  Well, smart people are often kind of\\ncrazy.But maybe there is a simpler explanation. Maybe\"),\n",
       "  Document(metadata={'source': '../../data/paul_essay.txt'}, page_content='the smartness and the craziness were not as separate\\nas we think. Physics seems to us a promising thing\\nto work on, and alchemy and theology obvious wastes\\nof time. But that\\'s because we know how things\\nturned out. In Newton\\'s day the three problems \\nseemed roughly equally promising. No one knew yet\\nwhat the payoff would be for inventing what we\\nnow call physics; if they had, more people would \\nhave been working on it. And alchemy and theology\\nwere still then in the category Marc Andreessen would \\ndescribe as \"huge, if true.\"Newton made three bets. One of them worked. But \\nthey were all risky.')],\n",
       " 'output_text': '\\n\\nBiographies of famous scientists often exaggerate their successes, leading to a misconception that their decisions were always rational and free from risks. This can be misleading as it overlooks their failures and the fact that their successful contributions may have appeared less risky due to conventional wisdom. The author suggests that subjective judgments about what is promising or wasteful can blur the lines between smartness and craziness, and that taking risks is essential for success in any pursuit, whether deemed smart or crazy, as outcomes are inherently unpredictable.'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
    "chain.invoke(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
